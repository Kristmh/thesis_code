{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset extraction \n",
    "This notebook will:\n",
    "* Find github issues by label\n",
    "* Produce one dataset with the given label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder path\n",
    "dir_path = r'../../../csv/all_issues' ## Point to the extracted folder containing all the issues csv files\n",
    "\n",
    "# list to store files\n",
    "res = []\n",
    "\n",
    "# Iterate directory\n",
    "for path in os.listdir(dir_path):\n",
    "    # check if current path is a file\n",
    "    if os.path.isfile(os.path.join(dir_path, path)):\n",
    "        res.append(path)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "appended_data = []\n",
    "# Regular expression to capture various variations of \"high priority\"\n",
    "high_priority = r\"\\bhigh\\W*p(?:ri(?:o(?:rity)?)?)?\\b|\\bp(?:ri(?:o(?:rity)?)?)?\\W*high\\b\"\n",
    "\n",
    "not_high_priority = r\"\\b(?:high\\W*|critical\\W*|severe\\W*|important\\W*|urgent\\W*|essential\\W*|imperative\\W*|paramount\\W*|pressing\\W*|crucial\\W*|vital\\W*|mandatory\\W*|top\\W*priority\\W*|compulsory\\W*|expedient\\W*)(?:p(?:ri(?:o(?:rity)?)?)?|\\burgent\\b|\\bsevere\\b)\\b|\\b(?:p(?:ri(?:o(?:rity)?)?)?|\\burgent\\b|\\bsevere\\b)\\W*(?:high|critical|severe|important|urgent|essential|imperative|paramount|pressing|crucial|vital|mandatory|top\\W*priority|compulsory|expedient)\\b\"\n",
    "medium_priority = r\"\\b(?:medium|mid)\\W*p(?:ri(?:o(?:rity)?)?)?\\b|\\bp(?:ri(?:o(?:rity)?)?)?\\W*(?:medium|mid)\\b\"\n",
    "\n",
    "low_priority = r\"\\blow\\W*p(?:ri(?:o(?:rity)?)?)?\\b|\\bp(?:ri(?:o(?:rity)?)?)?\\W*low\\b\"\n",
    "\n",
    "\n",
    "# Change to the pattern you want to extract\n",
    "pattern=high_priority\n",
    "file_name = \"high_priority\"\n",
    "\n",
    "length_res = len(res)\n",
    "for i, r in enumerate(res):\n",
    "    try:\n",
    "        file_path = f\"{dir_path}/{r}\"\n",
    "        df = pd.read_csv(file_path, index_col=0)\n",
    "        \n",
    "        # Make sure the labels column is not empty \n",
    "        df = df[df['labels'].notnull() & df['labels'].str.strip().astype(bool)]\n",
    "\n",
    "        df = df[\n",
    "           df[\"labels\"].str.contains(pattern, case=False, na=False, regex=True)\n",
    "        ]\n",
    "\n",
    "        if not df.empty:  # Append non-empty dataframes to the list\n",
    "           appended_data.append(df)\n",
    "\n",
    "\n",
    "        print(f\"{i}/{length_res} Processed: {r} \")\n",
    "    except Exception as e:\n",
    "        print(f\"{i}/{length_res} Error processing {r}: {e}\")\n",
    "\n",
    "appended_data = pd.concat(appended_data, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "appended_data = pd.DataFrame(appended_data)\n",
    "appended_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show top labels\n",
    "appended_data.labels.value_counts().to_frame()[:50] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repos with most issues\n",
    "appended_data.repo.value_counts().to_frame()[:50] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of unique repos\n",
    "df[\"repo\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create csv dir if it does not exist\n",
    "path_dir = \"csv\"\n",
    "if not os.path.exists(path_dir):\n",
    "    os.makedirs(path_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to csv\n",
    "priority_file_name = f\"csv/{file_name}_full_dataset.csv\"\n",
    "appended_data.to_csv(priority_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read csv to check if not corrupted\n",
    "pri = pd.read_csv(priority_file_name, index_col=0)\n",
    "pri"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "a64f21293159cd9c4e596ef7fd6c17a9c99d13712885c299cb3370e7a4d97830"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
