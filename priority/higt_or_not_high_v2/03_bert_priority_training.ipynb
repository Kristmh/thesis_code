{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f89275a9",
   "metadata": {},
   "source": [
    "# Importsand preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d342db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import datasets\n",
    "import transformers\n",
    "from GPUtil import showUtilization as gpu_usage\n",
    "from numba import cuda\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "\n",
    "from datasets import load_dataset, Dataset, DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b3abce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !watch -n 0.5 nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6579d707",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'PyTorch version: {torch.__version__}')  # 1.9.1+cu111\n",
    "print(f'CUDA version: {torch.version.cuda}')  # 11.1\n",
    "print(f'cuDNN version: {torch.backends.cudnn.version()}')  # 8005\n",
    "print(f'Current device: {torch.cuda.current_device()}')  # 0\n",
    "print(f'Is cuda available: {torch.cuda.is_available()}')  # TRUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30504d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Transformers version: {transformers.__version__}')\n",
    "print(f'Datasets version: {datasets.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f96c096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prevent a warning related to the tokenization process in the transformers library. \n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"False\"\n",
    "# Makes CUDA operations synchronous\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855ee0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the GPU with the least memory usage.\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72ee9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def free_gpu_cache():\n",
    "    print(\"Initial GPU Usage\")\n",
    "    gpu_usage()                             \n",
    "\n",
    "    # free unreferenced tensors from the GPU memory.\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    cuda.select_device(0)\n",
    "    cuda.close()\n",
    "    cuda.select_device(0)\n",
    "\n",
    "    print(\"GPU Usage after emptying the cache\")\n",
    "    gpu_usage()\n",
    "\n",
    "free_gpu_cache()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc89bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"csv/clean_priority_high_or_not_high.csv\" , index_col = 0)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test_set = pd.read_csv(\"testset_priority_high_or_not_high_clean.csv\" , index_col = 0)\n",
    "data_test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26fa5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smaller and faster than bert.\n",
    "base_model_id = \"distilbert-base-uncased\"\n",
    "\n",
    "epochs = 5 #Number of full cyles through the training set.\n",
    "num_labels = 2 #Number of labels, high, med, low priority.\n",
    "learning_rate = 5e-5 # Rate the model updates based on the data its trained on.\n",
    "train_batch_size = 16 # Number of training examples in one iteration.\n",
    "eval_batch_size = 32 # Number evalutaion examples in on iteratoion.\n",
    "save_strategy = \"no\" # Should the model be saved automatically during training.\n",
    "save_steps = 500 # How often to save the model during training. No effect since no over.\n",
    "logging_steps = 100\n",
    "model_dir = \"./model\" #Where to save model\n",
    "\n",
    "# Use early stopping to prevent overfitting\n",
    "#load_best_model_at_end=True\n",
    "#metric_for_best_model=\"eval_loss\"\n",
    "#greater_is_better=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48ccefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split dataframe into three parts: training, validation and testing.\n",
    "def train_validate_test_split(df, train_percent=.8, validate_percent=.1, seed=42):\n",
    "    np.random.seed(seed)\n",
    "    # Shuffle index of dataframe\n",
    "    perm = np.random.permutation(df.index)\n",
    "    \n",
    "    df_length = len(df.index)\n",
    "    \n",
    "    # Number of row in training set\n",
    "    train_end = int(train_percent * df_length)\n",
    "    # Number of rows in validate set\n",
    "    validate_end = int(validate_percent * df_length) + train_end\n",
    "    \n",
    "    # From start to train end\n",
    "    train = df.iloc[perm[:train_end]]\n",
    "    # From train_end to validate_end\n",
    "    validate = df.iloc[perm[train_end:validate_end]]\n",
    "    # From validate to the last row in dataframe.\n",
    "    test = df.iloc[perm[validate_end:]]\n",
    "    return train, validate, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830cb234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drops rows with missing values\n",
    "data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365a025e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resets the index after dropping rows\n",
    "data.reset_index(inplace=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29193757",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drops the index col, better for managint the data.\n",
    "data.drop(columns= [\"index\"], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d65133a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0e425f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 80% trainig, 10% validate, 10% test. Seed 42.\n",
    "# Test 80-10-10 and 70-15-15\n",
    "train , validate , test = train_validate_test_split(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024bdd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.set_index(\"label\" , inplace = True)\n",
    "validate.set_index(\"label\" , inplace = True)\n",
    "test.set_index(\"label\" , inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb16842",
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32ee5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert from Pandas DataFrame to Hugging Face datasets\n",
    "tds = Dataset.from_pandas(train)\n",
    "vds = Dataset.from_pandas(validate)\n",
    "testds = Dataset.from_pandas(test)\n",
    "\n",
    "separate_test_set = Dataset.from_pandas(data_test_set)\n",
    "ds = DatasetDict()\n",
    "\n",
    "ds[\"test\"] = testds\n",
    "ds[\"train\"] = tds\n",
    "ds[\"validate\"] = vds\n",
    "ds[\"separate_test_set\"] = separate_test_set\n",
    "\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386b14ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ds[\"train\"]\n",
    "valid_dataset = ds[\"validate\"]\n",
    "test_ds = ds[\"test\"]\n",
    "separate_test_set_dataset = ds[\"separate_test_set\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3263a36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ef1614",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\"accuracy\": acc, \"f1\": f1, \"precision\": precision, \"recall\": recall}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbffc393",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(base_model_id, num_labels=num_labels)\n",
    "#tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n",
    "# optim = torch.optim.Adam(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484feadf",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90b52d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ckpt = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76dfffd",
   "metadata": {},
   "source": [
    "    Tokenizing the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c848144",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize the dataset to the correct input for the transformer model.\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"text_clean\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize, batched=True, batch_size=len(train_dataset))\n",
    "valid_dataset = valid_dataset.map(tokenize, batched=True, batch_size=len(valid_dataset))\n",
    "test_dataset = test_ds.map(tokenize, batched=True, batch_size=len(test_ds))\n",
    "separate_test_set_dataset = separate_test_set_dataset.map(tokenize, batched=True, batch_size=len(separate_test_set_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10c7769",
   "metadata": {},
   "source": [
    "## Training a classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5a7f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=model_dir,\n",
    "    num_train_epochs=epochs,\n",
    "    per_device_train_batch_size=train_batch_size,\n",
    "    per_device_eval_batch_size=eval_batch_size,\n",
    "    save_strategy=save_strategy,\n",
    "    save_steps=save_steps,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=learning_rate,\n",
    "    logging_steps=logging_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318fc8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    " trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5578f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ac7267",
   "metadata": {},
   "source": [
    "* Training loss: Difference between the predictons made by the model on the training dataset vs on the actual data.\n",
    "* Validation loss: how well the model functions on unseen data.\n",
    "* Accuracy: How much the model gets correct. number of correct Prediction / total number of predictions.\n",
    "* F1: consider both precision and recall. \n",
    "* Precision: Accuracy of positive predictions. Percison TP = TP + FP. How often the model is correct.\n",
    "* Recall: True positive rate. how many items the model gets correct from the total amount."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c34a7fd",
   "metadata": {},
   "source": [
    "### Training loss decreases, valdiation loss increases = Overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a791423d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate validation set\n",
    "eval_result = trainer.evaluate(eval_dataset=valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88295878",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in sorted(eval_result.items()):\n",
    "    print(f\"{key} = {value}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce22454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate test data set\n",
    "test_results = trainer.evaluate(eval_dataset=test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5970cfc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in sorted(test_results.items()):\n",
    "    print(f\"{key} = {value}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "separate_test_set_results = trainer.evaluate(eval_dataset=separate_test_set_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in sorted(separate_test_set_results.items()):\n",
    "    print(f\"{key} = {value}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52c8395",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(model_dir + \"_local\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac76c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "    \n",
    "classifier = pipeline(\"text-classification\", model=\"./model_local\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534615c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee71c0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier(\"this does not need to be done fast\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3978b8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier(\"this is super important\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb83892",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier(\"this bug has super high impact on the project\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a082ef",
   "metadata": {},
   "source": [
    "## Important to delete large objects to free memory \n",
    "del train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44bc1927",
   "metadata": {},
   "outputs": [],
   "source": [
    "del valid_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d547dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff22f3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free cache\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2be48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ff4d6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
