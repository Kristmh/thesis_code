{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract data from public Jira data\n",
    "* This notebook will extract data from the public jira dataset\n",
    "* The dataset is stored in mongoDB.\n",
    "* mongoDB must be installed and running on your system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more details refer to \n",
    " https://zenodo.org/record/5901956"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Command used to export the data (this command takes about 15 minutes to complete).\n",
    "\n",
    "`mongodump --db=JiraRepos --gzip --archive=mongodump-JiraRepos.archive`\n",
    "\n",
    "Accompanying command to restore the data (this command takes about 15 minutes to complete). Expanded, this data is ~60GB inside MongoDB.\n",
    "\n",
    "`mongorestore --gzip --archive=mongodump-JiraRepos.archive --nsFrom \"JiraRepos.*\" --nsTo \"JiraRepos.*\"`\n",
    "\n",
    "Change the `--nsTo` command to contain the desired name for the JiraRepos database.\n",
    "mongorestore --gzip --archive=mongodump-JiraRepos.archive --nsFrom \"JiraRepos.*\" --nsTo \"JiraRepos.Apache\"\n",
    "\n",
    "\n",
    "For more information see: https://docs.mongodb.com/manual/tutorial/backup-and-restore-tools/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jira Dataset for TD filtered was extracted from https://zenodo.org/record/5901956  (https://arxiv.org/pdf/2201.08368.pdf) and adapted \n",
    "\n",
    "Montgomery, Lloyd, LÃ¼ders, Clara, & Maalej, Prof. Dr. Walid. (2022). The Public Jira Dataset [Data set]. Zenodo. https://doi.org/10.5281/zenodo.5901956\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "# Default connection to localhost\n",
    "myclient = pymongo.MongoClient(\"mongodb://localhost:27017/\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Database(MongoClient(host=['localhost:27017'], document_class=dict, tz_aware=False, connect=True), 'JiraRepos')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mydb = myclient[\"JiraRepos\"]\n",
    "mydb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Spring',\n",
       " 'RedHat',\n",
       " 'Sakai',\n",
       " 'JiraEcosystem',\n",
       " 'Jira',\n",
       " 'Hyperledger',\n",
       " 'Apache',\n",
       " 'SecondLife',\n",
       " 'MariaDB',\n",
       " 'MongoDB',\n",
       " 'Mojang',\n",
       " 'Qt',\n",
       " 'JFrog',\n",
       " 'IntelDAOS',\n",
       " 'Mindville',\n",
       " 'Sonatype']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collist = mydb.list_collection_names()\n",
    "collist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name of the collection to extract\n",
    "collection_name = 'MariaDB'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed chunk 1 of 63\n",
      "Processed chunk 2 of 63\n",
      "Processed chunk 3 of 63\n",
      "Processed chunk 4 of 63\n",
      "Processed chunk 5 of 63\n",
      "Processed chunk 6 of 63\n",
      "Processed chunk 7 of 63\n",
      "Processed chunk 8 of 63\n",
      "Processed chunk 9 of 63\n",
      "Processed chunk 10 of 63\n",
      "Processed chunk 11 of 63\n",
      "Processed chunk 12 of 63\n",
      "Processed chunk 13 of 63\n",
      "Processed chunk 14 of 63\n",
      "Processed chunk 15 of 63\n",
      "Processed chunk 16 of 63\n",
      "Processed chunk 17 of 63\n",
      "Processed chunk 18 of 63\n",
      "Processed chunk 19 of 63\n",
      "Processed chunk 20 of 63\n",
      "Processed chunk 21 of 63\n",
      "Processed chunk 22 of 63\n",
      "Processed chunk 23 of 63\n",
      "Processed chunk 24 of 63\n",
      "Processed chunk 25 of 63\n",
      "Processed chunk 26 of 63\n",
      "Processed chunk 27 of 63\n",
      "Processed chunk 28 of 63\n",
      "Processed chunk 29 of 63\n",
      "Processed chunk 30 of 63\n",
      "Processed chunk 31 of 63\n",
      "Processed chunk 32 of 63\n",
      "Processed chunk 33 of 63\n",
      "Processed chunk 34 of 63\n",
      "Processed chunk 35 of 63\n",
      "Processed chunk 36 of 63\n",
      "Processed chunk 37 of 63\n",
      "Processed chunk 38 of 63\n",
      "Processed chunk 39 of 63\n",
      "Processed chunk 40 of 63\n",
      "Processed chunk 41 of 63\n",
      "Processed chunk 42 of 63\n",
      "Processed chunk 43 of 63\n",
      "Processed chunk 44 of 63\n",
      "Processed chunk 45 of 63\n",
      "Processed chunk 46 of 63\n",
      "Processed chunk 47 of 63\n",
      "Processed chunk 48 of 63\n",
      "Processed chunk 49 of 63\n",
      "Processed chunk 50 of 63\n",
      "Processed chunk 51 of 63\n",
      "Processed chunk 52 of 63\n",
      "Processed chunk 53 of 63\n",
      "Processed chunk 54 of 63\n",
      "Processed chunk 55 of 63\n",
      "Processed chunk 56 of 63\n",
      "Processed chunk 57 of 63\n",
      "Processed chunk 58 of 63\n",
      "Processed chunk 59 of 63\n",
      "Processed chunk 60 of 63\n",
      "Processed chunk 61 of 63\n",
      "Processed chunk 62 of 63\n",
      "Processed chunk 63 of 63\n",
      "All chunks processed.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pymongo as pm\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "CHUNK_SIZE = 500\n",
    "client = pm.MongoClient()\n",
    "coll = client.get_database('JiraRepos').get_collection(collection_name)\n",
    "cursor = coll.find({}, batch_size=CHUNK_SIZE)\n",
    "\n",
    "# Count total documents and calculate total chunks\n",
    "total_docs = coll.count_documents({})\n",
    "total_chunks = (total_docs + CHUNK_SIZE - 1) // CHUNK_SIZE  # Rounds up the division\n",
    "\n",
    "def yield_rows(cursor, chunk_size):\n",
    "    \"\"\"\n",
    "    Generator to yield chunks from cursor\n",
    "    :param cursor:\n",
    "    :param chunk_size:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    chunk = []\n",
    "    for i, row in enumerate(cursor):\n",
    "        if i % chunk_size == 0 and i > 0:\n",
    "            yield chunk\n",
    "            del chunk[:]\n",
    "        chunk.append(row)\n",
    "    yield chunk\n",
    "\n",
    "# Define the columns you wish to extract\n",
    "desired_columns = [\n",
    "    \"id\", \n",
    "    \"fields.project.name\", \n",
    "    \"fields.priority.name\", \n",
    "    \"fields.created\", \n",
    "    \"fields.labels\", \n",
    "    \"fields.summary\", \n",
    "    \"fields.description\", \n",
    "    \"fields.status.name\",\n",
    "    \"fields.status.description\", \n",
    "    \"fields.issuetype.name\", \n",
    "    \"fields.issuetype.description\", \n",
    "    \"fields.issuetype.subtask\", \n",
    "    \"fields.comments\"\n",
    "]\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs(\"dataset_dump/\" + collection_name, exist_ok=True)\n",
    "\n",
    "chunks = yield_rows(cursor, CHUNK_SIZE)\n",
    "\n",
    "# Initialize chunk counter\n",
    "chunk_counter = 0\n",
    "\n",
    "for chunk in chunks:\n",
    "    chunk_counter += 1  # Increment the chunk counter\n",
    "    df = pd.json_normalize(chunk, errors='ignore')\n",
    "    \n",
    "    # Select only the columns that exist in the DataFrame\n",
    "    available_columns = [col for col in desired_columns if col in df.columns]\n",
    "    df = df[available_columns]\n",
    "\n",
    "    # Save to CSV, considering 'id' is always present\n",
    "    df.to_csv(f\"dataset_dump/{collection_name}/{collection_name}-{chunk_counter}.csv\", index=False)\n",
    "\n",
    "    print(f\"Processed chunk {chunk_counter} of {total_chunks}\")\n",
    "\n",
    "# Print completion message\n",
    "print(\"All chunks processed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<dask_expr.expr.DataFrame: expr=ReadCSV(5e2ecf6)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dask\n",
    "dask.config.set({'dataframe.query-planning': True})\n",
    "import dask.dataframe as dd\n",
    "df = dd.read_csv(f'dataset_dump/{collection_name}/{collection_name}-*.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Mismatched dtypes found in `pd.read_csv`/`pd.read_table`.\n\n+---------------------------+--------+----------+\n| Column                    | Found  | Expected |\n+---------------------------+--------+----------+\n| fields.status.description | object | float64  |\n+---------------------------+--------+----------+\n\nThe following columns also raised exceptions on conversion:\n\n- fields.status.description\n  ValueError(\"could not convert string to float: 'This issue is not being actively worked on at the moment.'\")\n\nUsually this is due to dask's dtype inference failing, and\n*may* be fixed by specifying dtypes manually by adding:\n\ndtype={'fields.status.description': 'object'}\n\nto the call to `read_csv`/`read_table`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/thesis/lib/python3.11/site-packages/dask_expr/_collection.py:418\u001b[0m, in \u001b[0;36mFrameBase.compute\u001b[0;34m(self, fuse, **kwargs)\u001b[0m\n\u001b[1;32m    416\u001b[0m     out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mrepartition(npartitions\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    417\u001b[0m out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39moptimize(fuse\u001b[38;5;241m=\u001b[39mfuse)\n\u001b[0;32m--> 418\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDaskMethodsMixin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/thesis/lib/python3.11/site-packages/dask/base.py:375\u001b[0m, in \u001b[0;36mDaskMethodsMixin.compute\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    352\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute this dask collection\u001b[39;00m\n\u001b[1;32m    353\u001b[0m \n\u001b[1;32m    354\u001b[0m \u001b[38;5;124;03m    This turns a lazy Dask collection into its in-memory equivalent.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;124;03m    dask.compute\u001b[39;00m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 375\u001b[0m     (result,) \u001b[38;5;241m=\u001b[39m \u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraverse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    376\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/.conda/envs/thesis/lib/python3.11/site-packages/dask/base.py:661\u001b[0m, in \u001b[0;36mcompute\u001b[0;34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[0m\n\u001b[1;32m    658\u001b[0m     postcomputes\u001b[38;5;241m.\u001b[39mappend(x\u001b[38;5;241m.\u001b[39m__dask_postcompute__())\n\u001b[1;32m    660\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m shorten_traceback():\n\u001b[0;32m--> 661\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mschedule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdsk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    663\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m repack([f(r, \u001b[38;5;241m*\u001b[39ma) \u001b[38;5;28;01mfor\u001b[39;00m r, (f, a) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(results, postcomputes)])\n",
      "File \u001b[0;32m~/.conda/envs/thesis/lib/python3.11/site-packages/dask/dataframe/io/csv.py:142\u001b[0m, in \u001b[0;36mCSVFunctionWrapper.__call__\u001b[0;34m(self, part)\u001b[0m\n\u001b[1;32m    139\u001b[0m         rest_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124musecols\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m columns\n\u001b[1;32m    141\u001b[0m \u001b[38;5;66;03m# Call `pandas_read_text`\u001b[39;00m\n\u001b[0;32m--> 142\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpandas_read_text\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrest_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwrite_header\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menforce\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m project_after_read:\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns]\n",
      "File \u001b[0;32m~/.conda/envs/thesis/lib/python3.11/site-packages/dask/dataframe/io/csv.py:197\u001b[0m, in \u001b[0;36mpandas_read_text\u001b[0;34m(reader, b, header, kwargs, dtypes, columns, write_header, enforce, path)\u001b[0m\n\u001b[1;32m    195\u001b[0m df \u001b[38;5;241m=\u001b[39m reader(bio, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtypes:\n\u001b[0;32m--> 197\u001b[0m     \u001b[43mcoerce_dtypes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtypes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m enforce \u001b[38;5;129;01mand\u001b[39;00m columns \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mlist\u001b[39m(df\u001b[38;5;241m.\u001b[39mcolumns) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlist\u001b[39m(columns)):\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumns do not match\u001b[39m\u001b[38;5;124m\"\u001b[39m, df\u001b[38;5;241m.\u001b[39mcolumns, columns)\n",
      "File \u001b[0;32m~/.conda/envs/thesis/lib/python3.11/site-packages/dask/dataframe/io/csv.py:298\u001b[0m, in \u001b[0;36mcoerce_dtypes\u001b[0;34m(df, dtypes)\u001b[0m\n\u001b[1;32m    294\u001b[0m rule \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m61\u001b[39m)\n\u001b[1;32m    295\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMismatched dtypes found in `pd.read_csv`/`pd.read_table`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\n\u001b[1;32m    296\u001b[0m     rule\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, [dtype_msg, date_msg]))\n\u001b[1;32m    297\u001b[0m )\n\u001b[0;32m--> 298\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n",
      "\u001b[0;31mValueError\u001b[0m: Mismatched dtypes found in `pd.read_csv`/`pd.read_table`.\n\n+---------------------------+--------+----------+\n| Column                    | Found  | Expected |\n+---------------------------+--------+----------+\n| fields.status.description | object | float64  |\n+---------------------------+--------+----------+\n\nThe following columns also raised exceptions on conversion:\n\n- fields.status.description\n  ValueError(\"could not convert string to float: 'This issue is not being actively worked on at the moment.'\")\n\nUsually this is due to dask's dtype inference failing, and\n*may* be fixed by specifying dtypes manually by adding:\n\ndtype={'fields.status.description': 'object'}\n\nto the call to `read_csv`/`read_table`."
     ]
    }
   ],
   "source": [
    "df = df.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"fields.priority.name\"].value_counts().to_frame()[:50]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"fields.issuetype.name\"].value_counts().to_frame()[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"fields.project.name\"].value_counts().to_frame()[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of different projects\n",
    "df[\"fields.project.name\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To csv \n",
    "df.to_csv(f'final_dataset/{collection_name}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csv\n",
    "df1 = pd.read_csv(f'final_dataset/{collection_name}.csv')     \n",
    "df1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "a64f21293159cd9c4e596ef7fd6c17a9c99d13712885c299cb3370e7a4d97830"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
