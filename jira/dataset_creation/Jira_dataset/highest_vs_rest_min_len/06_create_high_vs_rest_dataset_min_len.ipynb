{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_files = \"/fp/projects01/ec307/ec-krimhau/github_datasets\"\n",
    "file_name = \"clean_github_repos_with_priority_issues.csv\"\n",
    "full_path = f'{path_to_files}/{file_name}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Save the combined DataFrame to a new CSV file\n",
    "df = pd.read_csv(f'{path_to_files}/{file_name}')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_with_nan = df[df['class'].isna()]\n",
    "rows_with_nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop useless classes\n",
    "df = df[~df['class'].str.contains(\"in_triage\")]\n",
    "df = df[~df['class'].str.contains(\"p2\")]\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping duplicate issues. \n",
    "df = df.drop_duplicates(subset='text')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bad repos with many bot issues \n",
    "bad_repos = ['AdguardTeam/AdguardFilters', 'webcompat/web-bugs', 'Predatoria/BeastsOfBermuda', 'Thy-Vipe/BeastsOfBermuda-issues' 'tomhughes/trac-tickets', 'k8smeetup/website-tasks', 'steedos/steedos-platform', 'msupply-foundation/mobile', 'StrangeLoopGames/EcoIssues']\n",
    "# Test sets \n",
    "test_sets = ['flutter/flutter', 'rust-lang/rust', 'python/mypy']\n",
    "remove_repos = bad_repos + test_sets\n",
    "df = df[~df['repo'].isin(remove_repos)]\n",
    "# Drop missing values from df\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Reset the index of df\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if any value in the 'repo' column is in the bad_repos list\n",
    "contains_bad_repo = df['repo'].isin(bad_repos).any()\n",
    "if contains_bad_repo == False:\n",
    "    print(f\"all repos removed. contains_bad_repo: {contains_bad_repo}\")\n",
    "else: \n",
    "    print(\"something went wrong when removing repos\")\n",
    "    contains_bad_repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df['text'] = df['text'].astype(str)\n",
    "\n",
    "# Calculate the length of each issue description\n",
    "df['issue_length'] = df['text'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate summary statistics for issue lengths\n",
    "summary_stats = df['issue_length'].describe()\n",
    "\n",
    "percentile_25th = int(summary_stats['25%'])\n",
    "print(percentile_25th)\n",
    "# Format and print the summary statistics for better readability\n",
    "print(\"Issue Lengths Summary Statistics:\")\n",
    "print(f\"Number of issues: {summary_stats['count']:.0f}\")\n",
    "print(f\"Mean: {summary_stats['mean']:.2f} characters\")\n",
    "print(f\"Standard Deviation: {summary_stats['std']:.2f} characters\")\n",
    "print(f\"Minimum Length: {summary_stats['min']:.0f} characters\")\n",
    "print(f\"25th Percentile: {summary_stats['25%']:.0f} characters\")\n",
    "print(f\"Median (50th Percentile): {summary_stats['50%']:.0f} characters\")\n",
    "print(f\"75th Percentile: {summary_stats['75%']:.0f} characters\")\n",
    "print(f\"Maximum Length: {summary_stats['max']:.0f} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Plotting the distribution of issue lengths with updated parameters\u001b[39;00m\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m8\u001b[39m))\n\u001b[0;32m----> 5\u001b[0m plt\u001b[38;5;241m.\u001b[39mhist(\u001b[43mdf\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124missue_length\u001b[39m\u001b[38;5;124m'\u001b[39m], bins\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mskyblue\u001b[39m\u001b[38;5;124m'\u001b[39m, edgecolor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblack\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mrange\u001b[39m\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m5000\u001b[39m])\n\u001b[1;32m      6\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDistribution of Issue Lengths (up to 5,000 characters)\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIssue Length (characters)\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting the distribution of issue lengths with updated parameters\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.hist(df['issue_length'], bins=100, color='skyblue', edgecolor='black', range=[0, 5000])\n",
    "plt.title('Distribution of Issue Lengths (up to 5,000 characters)')\n",
    "plt.xlabel('Issue Length (characters)')\n",
    "plt.ylabel('Number of issues')\n",
    "plt.axvline(df['issue_length'].mean(), color='red', linestyle='dashed', linewidth=1)\n",
    "plt.axvline(df['issue_length'].median(), color='orange', linestyle='dashed', linewidth=1)\n",
    "plt.legend(['Mean', 'Median', 'Issue Lengths'])\n",
    "plt.grid(True)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting the distribution of issue lengths with updated parameters\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.hist(df['issue_length'], bins=100, color='skyblue', edgecolor='black', range=[0, 5000])\n",
    "plt.title('Distribution of Issue Lengths (up to 5,000 characters)')\n",
    "plt.xlabel('Issue Length (characters)')\n",
    "plt.ylabel('Number of issues')\n",
    "plt.axvline(df['issue_length'].mean(), color='red', linestyle='dashed', linewidth=1)\n",
    "plt.axvline(df['issue_length'].median(), color='orange', linestyle='dashed', linewidth=1)\n",
    "plt.legend(['Mean', 'Median', 'Issue Lengths'])\n",
    "plt.grid(True)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the 25th percentile (199) chars a small length issues\n",
    "befor_len = len(df)\n",
    "df = df[df['issue_length'] >= percentile_25th]\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "after_len = len(df)\n",
    "diff = befor_len - after_len\n",
    "print(f\"Before: {befor_len}. After: {after_len}. Issues removed: {diff}\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting the distribution of issue lengths with updated parameters\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.hist(df['issue_length'], bins=100, color='skyblue', edgecolor='black', range=[0, 5000])\n",
    "plt.title('Distribution of Issue Lengths after removing 25% of min len (up to 5,000 characters)')\n",
    "plt.xlabel('Issue Length (characters)')\n",
    "plt.ylabel('Number of issues')\n",
    "plt.axvline(df['issue_length'].mean(), color='red', linestyle='dashed', linewidth=1)\n",
    "plt.axvline(df['issue_length'].median(), color='orange', linestyle='dashed', linewidth=1)\n",
    "plt.legend(['Mean', 'Median', 'Issue Lengths'])\n",
    "plt.grid(True)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate summary statistics for issue lengths\n",
    "summary_stats = df['issue_length'].describe()\n",
    "\n",
    "percentile_25th = int(summary_stats['25%'])\n",
    "print(percentile_25th)\n",
    "# Format and print the summary statistics for better readability\n",
    "print(\"Issue Lengths Summary Statistics:\")\n",
    "print(f\"Number of issues: {summary_stats['count']:.0f}\")\n",
    "print(f\"Mean: {summary_stats['mean']:.2f} characters\")\n",
    "print(f\"Standard Deviation: {summary_stats['std']:.2f} characters\")\n",
    "print(f\"Minimum Length: {summary_stats['min']:.0f} characters\")\n",
    "print(f\"25th Percentile: {summary_stats['25%']:.0f} characters\")\n",
    "print(f\"Median (50th Percentile): {summary_stats['50%']:.0f} characters\")\n",
    "print(f\"75th Percentile: {summary_stats['75%']:.0f} characters\")\n",
    "print(f\"Maximum Length: {summary_stats['max']:.0f} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count of issues per repo\n",
    "repo_names = df['repo'].value_counts().to_frame()[:50].index.to_numpy()\n",
    "print(repo_names)\n",
    "df['repo'].value_counts().to_frame()[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repos with most high priority labels.\n",
    "high_priority_repos = df[df['class'] == 'high']['repo'].value_counts().to_frame()[:50]\n",
    "high_priority_repos_only = df[df['class'] == 'high']['repo'].value_counts().nlargest(50).index.to_list()\n",
    "print(high_priority_repos_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p2\n",
    "df.loc[df[\"class\"] == \"p2\", 'labels'].value_counts().to_frame().head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Critical value count\n",
    "# Maybe remove the blocker since it is not always about priority.\n",
    "df.loc[df['class'] == 'critical', 'labels'].value_counts().to_frame().head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top labels for each priority level\n",
    "df.loc[df['class'] == 'high', 'labels'].value_counts().to_frame().head(50)\n",
    "# Print all labels for high priority\n",
    "#pd.set_option('display.max_rows', None)\n",
    "#df.loc[df['class'] == 'High', 'labels'].value_counts().to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Medium\n",
    "df.loc[df['class'] == 'medium', 'labels'].value_counts().to_frame().head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Low\n",
    "df.loc[df['class'] == 'low', 'labels'].value_counts().to_frame().head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# non_priority\n",
    "df.loc[df['class'] == 'non_priority', 'labels'].value_counts().to_frame().head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in_triage\n",
    "df.loc[df['class'] == 'in_triage', 'labels'].value_counts().to_frame().head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique repo\n",
    "df['repo'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value count labels\n",
    "df[\"labels\"].value_counts().to_frame()[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.value_counts('class').to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high = df[df['class'] == 'high']\n",
    "high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "critical = df[df['class'] == 'critical']\n",
    "critical "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p2_high = df[df['class'] == 'p2_high']\n",
    "p2_high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low = df[df['class'] == 'low']\n",
    "low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "medium = df[df['class'] == 'medium']\n",
    "medium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_priority = df[df['class'] == 'non_priority']\n",
    "non_priority"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_priority = pd.concat([critical, high, p2_high], ignore_index=True)\n",
    "high_priority[\"label\"] = 1\n",
    "high_priority"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rest = pd.concat([non_priority, low, medium], ignore_index=True)\n",
    "rest[\"label\"] = 0\n",
    "rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "# Number of rows in the highest class\n",
    "n_high = high_priority.shape[0]\n",
    "\n",
    "rest_sampled = rest.sample(n=n_high, random_state=42)  \n",
    "\n",
    "\n",
    "balanced_data = pd.concat([high_priority, rest_sampled])\n",
    "\n",
    "# Shuffle the combined dataset\n",
    "balanced_data = shuffle(balanced_data, random_state=42)\n",
    "balanced_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "balanced_data['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_data['labels'].value_counts().to_frame()[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_data = balanced_data[[\"label\", \"text\"]]\n",
    "balanced_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_data['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().any().any()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = f'{path_to_files}/HP_vs_rest/high_vs_rest_min_len_25th_percentile.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_data.to_csv(file_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing = pd.read_csv(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
