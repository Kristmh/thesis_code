{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99340d49-8634-4ac7-9054-aeec3da78eaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fp/homes01/u01/ec-krimhau/.local/lib/python3.11/site-packages/transformers/convert_slow_tokenizer.py:550: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded and ready for inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel, AutoConfig, AutoTokenizer\n",
    "\n",
    "HASH_NAME = \"kmyc4k23vc47\"\n",
    "# Config for the model\n",
    "CONFIG = {\"seed\": 42,\n",
    "          \"epochs\": 3,\n",
    "          \"model_name\": \"microsoft/deberta-v3-base\",\n",
    "          \"train_batch_size\": 8,\n",
    "          \"valid_batch_size\": 16,\n",
    "          \"max_length\": 512,\n",
    "          \"learning_rate\": 1e-5,\n",
    "          \"scheduler\": 'CosineAnnealingLR',\n",
    "          \"min_lr\": 1e-6,\n",
    "          \"T_max\": 500,\n",
    "          \"weight_decay\": 1e-6,\n",
    "          \"n_fold\": 3,\n",
    "          \"n_accumulate\": 1,\n",
    "          \"num_classes\": 2,\n",
    "          \"device\": torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n",
    "          \"hash_name\": HASH_NAME,\n",
    "          \"_wandb_kernel\": \"deb\",\n",
    "          }\n",
    "\n",
    "CONFIG[\"tokenizer\"] = AutoTokenizer.from_pretrained(CONFIG['model_name'])\n",
    "CONFIG['group'] = f'{HASH_NAME}-Baseline'\n",
    "\n",
    "# Mean pooling class definition\n",
    "class MeanPooling(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MeanPooling, self).__init__()\n",
    "\n",
    "    def forward(self, last_hidden_state, attention_mask):\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n",
    "        sum_mask = input_mask_expanded.sum(1)\n",
    "        sum_mask = torch.clamp(sum_mask, min=1e-9)\n",
    "        mean_embeddings = sum_embeddings / sum_mask\n",
    "        return mean_embeddings\n",
    "\n",
    "# Model class definition\n",
    "class HP_Model(nn.Module):\n",
    "    def __init__(self, model_name):\n",
    "        super(HP_Model, self).__init__()\n",
    "        self.model = AutoModel.from_pretrained(model_name)\n",
    "        self.config = AutoConfig.from_pretrained(model_name)\n",
    "        self.drop = nn.Dropout(p=0.2)\n",
    "        self.pooler = MeanPooling()\n",
    "        self.fc = nn.Linear(self.config.hidden_size, CONFIG['num_classes'])  # Define CONFIG correctly\n",
    "\n",
    "    def forward(self, ids, mask):\n",
    "        out = self.model(input_ids=ids, attention_mask=mask, output_hidden_states=False)\n",
    "        out = self.pooler(out.last_hidden_state, mask)\n",
    "        out = self.drop(out)\n",
    "        outputs = self.fc(out)\n",
    "        return outputs\n",
    "\n",
    "# Initialize the model - replace 'model_name' with the actual model you used (like 'bert-base-uncased')\n",
    "model_name = 'microsoft/deberta-v3-base'\n",
    "model = HP_Model(model_name)\n",
    "\n",
    "# Load the saved weights\n",
    "model_path = 'kmyc4k23vc47-Loss-Fold-0.bin'\n",
    "model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "print(\"Model loaded and ready for inference.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94e3848b-dc07-4c8d-9f14-32cea0772b4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TD_Chromium_dataset_clean.csv  TD_dataset_clean.csv  TD_jira_TD_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "td_file_path = \"/fp/projects01/ec307/ec-krimhau/TD_dataset\"\n",
    "td_file_name = \"TD_Chromium_dataset_clean.csv\"\n",
    "TD_dataset = pd.read_csv(f\"{td_file_path}/{td_file_name}\", index_col = 0)\n",
    "!ls /fp/projects01/ec307/ec-krimhau/TD_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb2feaf8-bd5c-4444-b6e1-8a30b7d291d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>network throttling does not throttle uploads u...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>parallelize test execution to speed up buildbo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>netfilter switch to pure pullbased filter api ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>huge animated gifs can lead to scroll jank use...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>issues with pdf viewer and iframe useragent  w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1495</th>\n",
       "      <td>wtperf multi btree wtperf dump core mac issues...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1496</th>\n",
       "      <td>scrub dirty page rather evicting single page r...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1497</th>\n",
       "      <td>include src include wiredtigerext h problemati...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1498</th>\n",
       "      <td>java freed memory overwrite handle close cause...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1499</th>\n",
       "      <td>fix test case false positive  issues relating ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1500 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  label\n",
       "0     network throttling does not throttle uploads u...      1\n",
       "1     parallelize test execution to speed up buildbo...      1\n",
       "2     netfilter switch to pure pullbased filter api ...      1\n",
       "3     huge animated gifs can lead to scroll jank use...      1\n",
       "4     issues with pdf viewer and iframe useragent  w...      1\n",
       "...                                                 ...    ...\n",
       "1495  wtperf multi btree wtperf dump core mac issues...      0\n",
       "1496  scrub dirty page rather evicting single page r...      1\n",
       "1497  include src include wiredtigerext h problemati...      0\n",
       "1498  java freed memory overwrite handle close cause...      1\n",
       "1499  fix test case false positive  issues relating ...      0\n",
       "\n",
       "[1500 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TD_dataset = pd.read_csv(f\"{td_file_path}/{td_file_name}\", index_col = 0)\n",
    "TD_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008adee5-d299-4bff-a7af-89764028d1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "df = TD_dataset\n",
    "\n",
    "# Dataset class definition\n",
    "class TDDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len=512):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.texts = dataframe['text'].values  # Accessing the 'text' column\n",
    "        self.labels = dataframe['label'].values  # Optionally if you need labels for some purpose\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            return_token_type_ids=False,\n",
    "            return_attention_mask=True,\n",
    "            truncation=True\n",
    "        )\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)  # Ensure 'model_name' is defined correctly\n",
    "\n",
    "# Create dataset\n",
    "test_dataset = TDDataset(df, tokenizer)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Define the prediction function\n",
    "def predict(model, data_loader):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            ids = batch['ids'].to(torch.device('cpu'))  # Make sure the device matches your model's device\n",
    "            mask = batch['mask'].to(torch.device('cpu'))\n",
    "            outputs = model(ids, mask)\n",
    "            predictions.append(outputs)\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# Perform inference\n",
    "model.to(torch.device('cpu'))  # Adjust as necessary for GPU\n",
    "test_predictions = predict(model, test_loader)\n",
    "\n",
    "# Display the predictions\n",
    "print(test_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dece6f3-0eb9-4a90-95a5-a9350598fc68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
